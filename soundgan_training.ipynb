{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXJ9OE8qUs-s"
   },
   "source": [
    "# SoundGAN - Generating Audio with a DCGAN with dilated convolutions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbbUFB-DU9nB"
   },
   "source": [
    "## **Importing modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "qo2Pz0UrUUCv",
    "outputId": "293df3a5-7cb3-4832-8a09-34e2501aaacd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import all necessary libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "import keras\n",
    "import scipy\n",
    "from scipy.io import wavfile\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib\n",
    "import random\n",
    "import sys\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ik1Ce9bCUmRu",
    "outputId": "c45294f8-23b7-491b-b990-3cec4b2fc192"
   },
   "outputs": [],
   "source": [
    "## mount drive / this is to save results and to load data if using google colab\n",
    "#drive.mount(\"/content/gdrive\",force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importing modules for processing of data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "enIW_8SAwkwd"
   },
   "outputs": [],
   "source": [
    "#import my functions for importing/processing/plotting data\n",
    "\n",
    "import data_import_process\n",
    "from data_import_process import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define data source and import/process data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define source of data\n",
    "DATADIR= 'G:/Music/Drumkits/Hats and Rides/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7XHJ2pNMVqwx",
    "outputId": "37ddcab9-d61c-4b57-8055-4ca742a72a48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept files with 44100 and deleted 0 files with different sample rate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pasch\\Downloads\\SoundGAN\\data_import_process.py:32: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  samplerate, data = wavfile.read(path)\n"
     ]
    }
   ],
   "source": [
    "training_data= import_data(DATADIR,44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "kQLQ8SrYWMh4",
    "outputId": "990d69d3-80a2-4226-b258-53252df8f795",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length of samples for this dataset is 452662 and the average samples per wav file is 52949.45\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make histogram of data length before processing\n",
    "plot_lengths(training_data)\n",
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0a6brjupWTDN",
    "outputId": "d5c828d8-0480-4c48-b10e-b4298d07e6d0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization to values between -1 and +1 completed successfully \n",
      "\n",
      "Successfully stripped all beginning and end silences effectively reducing the average samples of the dataset from 52949.45 to 30575.35\n",
      "\n",
      "Files with length of over 7500 have been successfully removed \n",
      "\n",
      "Padding to achieve desired length \n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data= preprocess_data(training_data, 7500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "8nYQvaya7bbw",
    "outputId": "c3316758-f06c-42e6-f94a-c81a429b03d4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length of samples for this dataset is 7500 and the average samples per wav file is 7500.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make histogram of data after processing\n",
    "plot_lengths(training_data)\n",
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zIH7p6RMq17J",
    "outputId": "a8fde9a5-7994-4d78-9e35-ed4ad69f2dc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 7500, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "L7uLYQ4CWb3v"
   },
   "outputs": [],
   "source": [
    "### write back into wav file to see if i made mistakes in processing\n",
    "\n",
    "scipy.io.wavfile.write(\"processed_data_sample.wav\", 44100, training_data[0])\n",
    "\n",
    "##WORKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LD3BIhT5weeR"
   },
   "source": [
    "## **Define Neural Network Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1tp8eqJ9iTf3"
   },
   "outputs": [],
   "source": [
    "#generator model\n",
    "\n",
    "generator_model= keras.Sequential([\n",
    "keras.layers.Dense(120*256, input_shape=(120,)),\n",
    "\n",
    "keras.layers.Reshape((120, 256)),\n",
    "keras.layers.BatchNormalization(),\n",
    "keras.layers.ReLU(),\n",
    "\n",
    "keras.layers.UpSampling1D(size=16),\n",
    "keras.layers.Conv1D(128, 25, strides=4),\n",
    "keras.layers.BatchNormalization(),\n",
    "keras.layers.ReLU(),\n",
    "\n",
    "keras.layers.UpSampling1D(size=8),\n",
    "keras.layers.Conv1D(64, 25, strides=4),\n",
    "keras.layers.BatchNormalization(),\n",
    "keras.layers.ReLU(),\n",
    "\n",
    "keras.layers.UpSampling1D(size=8),\n",
    "keras.layers.Conv1D(32, 25, strides=4),\n",
    "keras.layers.BatchNormalization(),\n",
    "keras.layers.ReLU(),\n",
    "\n",
    "keras.layers.UpSampling1D(size=8),\n",
    "keras.layers.Conv1D(16, 25, strides=4),\n",
    "keras.layers.BatchNormalization(),\n",
    "keras.layers.ReLU(),\n",
    "\n",
    "keras.layers.UpSampling1D(size=8),\n",
    "keras.layers.Conv1D(1, 25, strides=4, activation='tanh',padding=\"same\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "RzWhCeNuUeL5"
   },
   "outputs": [],
   "source": [
    "#discriminator model\n",
    "\n",
    "discriminator_model= keras.Sequential([\n",
    "keras.layers.Conv1D(128, 25, strides=4, input_shape=[7500,1]),\n",
    "keras.layers.LeakyReLU(),\n",
    "keras.layers.Dropout(0.3),\n",
    "keras.layers.Conv1D(64, 25, strides=1,dilation_rate=2),\n",
    "keras.layers.LeakyReLU(),\n",
    "keras.layers.Dropout(0.3),\n",
    "keras.layers.Conv1D(64, 25, strides=1,dilation_rate=4),\n",
    "keras.layers.LeakyReLU(),\n",
    "keras.layers.Dropout(0.3),\n",
    "keras.layers.Conv1D(64, 25, strides=1,dilation_rate=8),\n",
    "keras.layers.LeakyReLU(),\n",
    "keras.layers.Dropout(0.3),\n",
    "keras.layers.Flatten(),\n",
    "keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DON'T COMPILE SINCE WE ARE USING A CUSTOM LOSS/OPTIMIZER/TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "S1XZ_m5cyeP7"
   },
   "outputs": [],
   "source": [
    "#RESHAPE DATA IN RIGHT SHAPE FOR INPUT INTO MODEL\n",
    "\n",
    "BATCH_SIZE=2\n",
    "train_dataset=reshape_data(training_data,7500,BATCH_SIZE,BUFFER_SIZE=training_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "eY6aQnXpUgWA"
   },
   "outputs": [],
   "source": [
    "#DEFINE BINARY CROSS-ENTROPY LOSS FOR DISCRIMINATOR AND GENERATOR\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "#DEFINE OPTIMIZER AND SET LEARNING RATE\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "1YfmanmUUoZC"
   },
   "outputs": [],
   "source": [
    "noise_dim=120\n",
    "\n",
    "@tf.function\n",
    "def train_step(sounds):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_sounds = generator_model(noise, training=True)\n",
    "\n",
    "      real_output = discriminator_model(sounds, training=True)\n",
    "      fake_output = discriminator_model(generated_sounds, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator_model.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator_model.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator_model.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator_model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to generate audio from generator model\n",
    "\n",
    "def generate_audio(amount,destination,noise_dim,sample_rate):\n",
    "  #check if destination folder exists, otherwise create\n",
    "  newpath = destination\n",
    "  if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)\n",
    "\n",
    "  for i in range(amount):\n",
    "    #sample and generate sound from latent space\n",
    "    noise = tf.random.normal([1, noise_dim])\n",
    "    generated_sound1 = generator_model(noise)\n",
    "    generated_sound1= generated_sound1[0][:]\n",
    "    generated_sound1_1 = np.asarray(generated_sound1, np.float32)\n",
    "    ### write back into wav file to see results\n",
    "    scipy.io.wavfile.write(str(destination)+str(i)+\".wav\", sample_rate, generated_sound1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "wrcD9J9zUubt"
   },
   "outputs": [],
   "source": [
    "#DEFINE TRAINING LOOP \n",
    "\n",
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for sound_batch in dataset:\n",
    "      train_step(sound_batch)\n",
    "\n",
    "    # generate and save 10 audio clips every 50 epochs\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "      generate_audio(10,\"generated/epoch{}/\".format(epoch + 1),120,44100)\n",
    "\n",
    "    print (\"\\r\",'Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start), end=\"         \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "0RpzxeFDEMcn",
    "outputId": "f94458f5-9fa8-4377-fd69-f7866109ac53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Time for epoch 50 is 2.9516680240631104 sec         "
     ]
    }
   ],
   "source": [
    "train(train_dataset, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GENERATE EXAMPLES WHEN DONE WITH MODEL\n",
    "\n",
    "generate_audio(100,\"generated/epoch{}/\".format(50),120,44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if Network is simply memorizing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DECLARE PATH TO GENERATED DATA THAT YOU WISH TO COMPARE\n",
    "DATADIR=\"generated/epoch50/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tPJFMqgM9Ddl",
    "outputId": "c45781da-6d55-4fb5-e765-2f57e6492e23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept files with 44100 and deleted 0 files with different sample rate\n"
     ]
    }
   ],
   "source": [
    "generated_data=import_data(DATADIR,44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9e37WY2uhFOJ",
    "outputId": "742cf31b-9f5d-4cbe-b870-81e2f8bbc1be"
   },
   "outputs": [],
   "source": [
    "#select audio file to compare\n",
    "file_to_compare= generated_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "s9NbgUIHifU0"
   },
   "outputs": [],
   "source": [
    "## THIS SAVES THE GENERATED DATA AND THE MOST SIMILAR TRAINING DATA. \n",
    "## THIS NEEDS TO BE COMPARED MANUALLY TO VERIFY IF MODEL IS MEMORIZING\n",
    "\n",
    "def export_similar(generated_file,training_data):\n",
    "    x=[]\n",
    "    x.append(generated_file)\n",
    "    x.append(training_data[find_most_similar_audio(generated_file,training_data)])\n",
    "    write_wav_data(x,'/similaraudio/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most similar audio in the dataset is number 2 (index nr. 1) and has an mse of 0.012512419\n"
     ]
    }
   ],
   "source": [
    "export_similar(file_to_compare,training_data)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "DILATED DCGAN FINAL THESIS",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
